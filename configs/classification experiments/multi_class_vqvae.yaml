# vqvae_pretraining.yaml


Tokenizer:
  basic_vqvae:
    LaBRAM:
      in_channels: 1
      emb_size: 64
      code_book_size: 512
      encoder_type: 'temporal_1d_conv'
      decoder_task: 'LaBRAM'
      token_patch_size: 1 #1s
      beta: 0.2 #0.2
      num_heads: 4
      depth: 4
      max_seq_len: 1024
    temporal_recon:
      in_channels: 1
      emb_size: 64
      code_book_size: 512
      encoder_type: 'temporal_1d_conv'
      decoder_task: 'temporal_recon'
      token_patch_size: 1 #1s
      beta: 0.2
      num_heads: 4
      depth: 4
      max_seq_len: 1024
    


  
Dataset:
  SHHS:
    sampling_freq: 125
    channels: [0,5] # only EEG channels
    num_channels: 2
    data_dir: '/srv/local/data/SHHS/processed_all'
    num_classes: 5
    vqvae_patch_size: 25
    vqvae_smallest_kernel_divider: 5
    vqvae_max_seq_len: 5
    vqvae_decoder_out_dim:
      LaBRAM: 62
      temporal_recon: 125


Training:
  batch_size: 256
  num_workers: 8
  experiment_path: '/home/jp65/Biosignals_Research/EEG_BPE_Experiments/vqvae_tokenization_classification'
  optimizer: AdamW
  lr: 0.0001 #1e-4
  weight_decay: 0.00001 #1e-5
  beta1: 0.9
  beta2: 0.99
  num_epochs: 5 #50  #50
    

